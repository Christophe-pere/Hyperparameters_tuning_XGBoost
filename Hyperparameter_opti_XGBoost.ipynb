{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Hyperparameters optimization XGBoost Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center><h2>Hypertuning, Training best parameters, save model, feature importance</h2></center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Contents</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#part_intro)\n",
    "- [I - Hyperparameters Optimization](#part_1)\n",
    "    - [1 - Libraries](#part_1_1)\n",
    "    - [2 - Load Data](#part_1_2)\n",
    "    - [3 - Stopwords data](#part_1_3)\n",
    "    - [4 - Preparation train test](#part_1_4)\n",
    "- [II - Hyperparameters](#part_2)\n",
    "    - [1 - RandomSearchCV](#part_2_1)\n",
    "    - [2 - GridSearchCV](#part_2_2)\n",
    "- [III - Best Model](#part_3)\n",
    "    - [III - 1. Train model with best parameters](#part_3_1)\n",
    "    - [III - 2. Save Model](#part_3_2)\n",
    "    - [III - 3. Load and test to compare metrics](#part_3_3)\n",
    "- [VI - Feature importance](#part_4)\n",
    "    - [Part VI - 1](#part_4_1)\n",
    "    - [Part VI - 2](#part_4_2)\n",
    "- [Conclusion](#part_conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_intro\">Introduction</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_1\">I - Hyperparameters Optimization</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a method for data treatment \n",
    "count_vect         = True\n",
    "tf_idf             = False\n",
    "tf_idf_ngram       = False\n",
    "tf_idf_ngram_char  = False\n",
    "concat_methods     = False # concat data representation\n",
    "random_search_model= True\n",
    "grid_search_model  = False\n",
    "num_gpu            = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "# Name file \n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_XGB_CLASSIFIER_MODEL     = \"XGBoost_classifier.sav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_1\">I 1 - Libraries</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "/home/chris/.local/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#imported libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sklearn\n",
    "# ---- Call tqdm to see progress bar with pandas\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_2\">I 2 - Load Data</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../projet_classification_mails/mails_clean_concat_ref_folders.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT   = \"mails\"\n",
    "LABEL  = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=[TEXT], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_3\">I 3 - Stopwords data</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count_vect:\n",
    "    stop_word = np.loadtxt(\"../stopwords/stopwords-fr.txt\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31382/31382 [03:06<00:00, 168.58it/s]\n"
     ]
    }
   ],
   "source": [
    "if count_vect:\n",
    "    def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)\n",
    "    \n",
    "    train.loc[:,TEXT+\"_sw\"] = train.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train[LABEL].isnull().sum()>0:\n",
    "    train.dropna(subset=[LABEL], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mails         0\n",
       "ref_folder    0\n",
       "label         0\n",
       "mails_sw      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-2758342f494e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[LABEL][train[LABEL]!=\"annulation\"] = \"other\"\n"
     ]
    }
   ],
   "source": [
    "train[LABEL][train[LABEL]!=\"annulation\"] = \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_4\">I 4 - Preparation train test</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# ML classic \n",
    "if count_vect:\n",
    "    train_x, test_x, y_train, y_test = train_test_split(train[TEXT+\"_sw\"], train[LABEL], random_state=42, stratify=train[LABEL], test_size=0.2)\n",
    "else:\n",
    "    train_x, test_x, y_train, y_test = train_test_split(train[TEXT], train[LABEL], random_state=42, stratify=train[LABEL], test_size=0.2)\n",
    "\n",
    "# Validation set\n",
    "train_x, valid_x, y_train, y_valid = train_test_split(train_x, y_train, random_state=42, stratify=y_train, test_size=0.2)\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(y_train)\n",
    "valid_y = encoder.fit_transform(y_valid)\n",
    "test_y = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20084,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_4\"><h3>One-Hot encoding (CountVectorizing)</h3></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 s, sys: 1.25 s, total: 21.6 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if count_vect:\n",
    "    # create a count vectorizer object \n",
    "    count_vect_ = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect_.fit(train[TEXT]+\"_sw\")\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain =  count_vect_.transform(train_x)\n",
    "    xvalid =  count_vect_.transform(valid_x)\n",
    "    xtest =  count_vect_.transform(test_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_5\"><h3>TF-IDF</h3></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if tf_idf:\n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect.transform(train_x)\n",
    "    xvalid =  tfidf_vect.transform(valid_x)\n",
    "    xtest  =  tfidf_vect.transform(test_x)\n",
    "    print(\"word level tf-idf done\")\n",
    "    \n",
    "if tf_idf_ngram:\n",
    "    # ngram level tf-idf \n",
    "    tfidf_vect_ngram_ = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram_.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect_ngram_.transform(train_x)\n",
    "    xvalid =  tfidf_vect_ngram_.transform(valid_x)\n",
    "    xtest =  tfidf_vect_ngram_.transform(test_x)\n",
    "    print(\"ngram level tf-idf done\")\n",
    "    \n",
    "if tf_idf_ngram_char:\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars_ = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) #token_pattern=r'\\w{1,}',\n",
    "    tfidf_vect_ngram_chars_.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect_ngram_chars_.transform(train_x) \n",
    "    xvalid =  tfidf_vect_ngram_chars_.transform(valid_x) \n",
    "    xtest  =  tfidf_vect_ngram_chars_.transform(test_x) \n",
    "    print(\"characters level tf-idf done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count_vect:\n",
    "    # save the model to disk\n",
    "    filename = NAME_COUNT_VECT_MODEL\n",
    "    pickle.dump(count_vect_, open(filename, 'wb'))\n",
    "\n",
    "if tf_idf:\n",
    "    # save the model to disk\n",
    "    filename = NAME_TF_IDF_MODEL\n",
    "    pickle.dump(tfidf_vect, open(filename, 'wb'))\n",
    "    \n",
    "if tf_idf_ngram:\n",
    "    # save the model to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_MODEL\n",
    "    pickle.dump(tfidf_ngram_, open(filename, 'wb'))\n",
    "    \n",
    "if tf_idf_ngram_char:\n",
    "    # save the model to disk\n",
    "    filename = NAME_TF_IDF_NGRAM_CHAR_MODEL\n",
    "    pickle.dump(tfidf_vect_ngram_chars_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_2\">II - Hyperparameters</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_2_1\">II 1 - RandomizedSearchCV</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 66.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 92.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed: 122.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 151.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 174.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.11352\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.09759\n",
      "[2]\tvalidation_0-error:0.09281\n",
      "[3]\tvalidation_0-error:0.07748\n",
      "[4]\tvalidation_0-error:0.07170\n",
      "[5]\tvalidation_0-error:0.07269\n",
      "[6]\tvalidation_0-error:0.07250\n",
      "[7]\tvalidation_0-error:0.06632\n",
      "[8]\tvalidation_0-error:0.06194\n",
      "[9]\tvalidation_0-error:0.06154\n",
      "[10]\tvalidation_0-error:0.06134\n",
      "[11]\tvalidation_0-error:0.06055\n",
      "[12]\tvalidation_0-error:0.06234\n",
      "[13]\tvalidation_0-error:0.06114\n",
      "[14]\tvalidation_0-error:0.05935\n",
      "[15]\tvalidation_0-error:0.05776\n",
      "[16]\tvalidation_0-error:0.05736\n",
      "[17]\tvalidation_0-error:0.05696\n",
      "[18]\tvalidation_0-error:0.05497\n",
      "[19]\tvalidation_0-error:0.05716\n",
      "[20]\tvalidation_0-error:0.05736\n",
      "[21]\tvalidation_0-error:0.05736\n",
      "[22]\tvalidation_0-error:0.05796\n",
      "[23]\tvalidation_0-error:0.05676\n",
      "[24]\tvalidation_0-error:0.05756\n",
      "[25]\tvalidation_0-error:0.05835\n",
      "[26]\tvalidation_0-error:0.05756\n",
      "[27]\tvalidation_0-error:0.05597\n",
      "[28]\tvalidation_0-error:0.05756\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-error:0.05497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if random_search_model:\n",
    "    #For classification \n",
    "\n",
    "    #Random Search\n",
    "    if num_gpu>0:\n",
    "        xgb_pipeline =XGBClassifier(tree_method= 'gpu_hist')\n",
    "    else:\n",
    "        xgb_pipeline =XGBClassifier()\n",
    "        \n",
    "    params = {\n",
    "            'learning_rate': [0.03, 0.01, 0.003, 0.001],\n",
    "            'min_child_weight': [1,3, 5,7, 10],\n",
    "            'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "            'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10],\n",
    "            'reg_lambda':np.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])}\n",
    "\n",
    "    fit_params = { \n",
    "            'early_stopping_rounds':10,\n",
    "            'eval_set':[(xvalid, valid_y)]}\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_pipeline, param_distributions=params, n_iter=1000,\n",
    "                                       scoring=\"precision\", n_jobs=-1,  verbose=3, random_state=42, cv=3 )\n",
    "    random_search.fit(xtrain,train_y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 1.0, 'reg_lambda': 0.8, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.03, 'gamma': 0, 'colsample_bytree': 0.6}\n",
      "0.9520525451559935\n"
     ]
    }
   ],
   "source": [
    "if random_search_model:\n",
    "    #Print out best parameters\n",
    "    print(random_search.best_params_)\n",
    "    #Print out scores on validation set\n",
    "    print(random_search.score(xtest,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_2_2\">II 2 - GridSearchCV</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0,\n",
       " 'reg_lambda': 0.8,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.03,\n",
       " 'gamma': 0,\n",
       " 'colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(int(10*random_search.best_params_[\"reg_lambda\"])-1, int(10*random_search.best_params_[\"reg_lambda\"])+1, 1))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 38.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 44.4min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 56.8min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 64.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 74.0min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 81.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 89.1min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 97.4min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 105.7min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 113.4min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 120.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 129.5min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 138.1min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 147.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 156.6min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 166.7min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 175.7min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 184.0min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 192.7min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 203.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 214.7min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 227.1min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 248.7min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 273.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 287.1min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 301.8min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed: 321.8min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed: 350.0min\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed: 370.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 382.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed: 392.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1097 tasks      | elapsed: 406.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 419.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed: 430.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 443.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed: 459.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1344 tasks      | elapsed: 469.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1397 tasks      | elapsed: 480.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 495.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1505 tasks      | elapsed: 516.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 534.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1617 tasks      | elapsed: 557.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1674 tasks      | elapsed: 572.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1733 tasks      | elapsed: 591.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 609.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1853 tasks      | elapsed: 629.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1914 tasks      | elapsed: 657.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 680.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed: 695.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2105 tasks      | elapsed: 715.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2170 tasks      | elapsed: 735.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2237 tasks      | elapsed: 759.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2304 tasks      | elapsed: 780.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2373 tasks      | elapsed: 800.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 825.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2513 tasks      | elapsed: 845.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 867.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2657 tasks      | elapsed: 893.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2730 tasks      | elapsed: 920.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2805 tasks      | elapsed: 943.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 tasks      | elapsed: 970.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2916 out of 2916 | elapsed: 982.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.11472\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.07568\n",
      "[2]\tvalidation_0-error:0.06871\n",
      "[3]\tvalidation_0-error:0.06652\n",
      "[4]\tvalidation_0-error:0.06692\n",
      "[5]\tvalidation_0-error:0.06433\n",
      "[6]\tvalidation_0-error:0.06393\n",
      "[7]\tvalidation_0-error:0.06055\n",
      "[8]\tvalidation_0-error:0.06373\n",
      "[9]\tvalidation_0-error:0.06214\n",
      "[10]\tvalidation_0-error:0.06254\n",
      "[11]\tvalidation_0-error:0.06174\n",
      "[12]\tvalidation_0-error:0.06075\n",
      "[13]\tvalidation_0-error:0.06035\n",
      "[14]\tvalidation_0-error:0.05875\n",
      "[15]\tvalidation_0-error:0.05736\n",
      "[16]\tvalidation_0-error:0.05776\n",
      "[17]\tvalidation_0-error:0.05716\n",
      "[18]\tvalidation_0-error:0.05955\n",
      "[19]\tvalidation_0-error:0.05756\n",
      "[20]\tvalidation_0-error:0.05975\n",
      "[21]\tvalidation_0-error:0.05855\n",
      "[22]\tvalidation_0-error:0.05895\n",
      "[23]\tvalidation_0-error:0.05975\n",
      "[24]\tvalidation_0-error:0.05875\n",
      "[25]\tvalidation_0-error:0.05835\n",
      "[26]\tvalidation_0-error:0.05776\n",
      "[27]\tvalidation_0-error:0.05736\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-error:0.05716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if grid_search_model:\n",
    "    #Grid Search\n",
    "\n",
    "    if num_gpu>0:\n",
    "        xgb_pipeline =XGBClassifier(tree_method= 'gpu_hist')\n",
    "    else:\n",
    "        xgb_pipeline =XGBClassifier()\n",
    "\n",
    "    gbm_param_grid = {\n",
    "        'learning_rate': np.array(range(int(100*random_search.best_params_[\"learning_rate\"])-1, int(100*random_search.best_params_[\"learning_rate\"])+2, 1))/100,\n",
    "        'subsample': np.array(range(int(10*random_search.best_params_[\"subsample\"])-1, int(10*random_search.best_params_[\"subsample\"])+2, 1))/10,\n",
    "        'reg_lambda': np.array(range(int(10*random_search.best_params_[\"reg_lambda\"])-1, int(10*random_search.best_params_[\"reg_lambda\"])+2, 1))/10,\n",
    "        'max_depth':np.array(range(random_search.best_params_[\"max_depth\"]-1, random_search.best_params_[\"max_depth\"]+3, 1)),\n",
    "        'colsample_bytree': np.array(range(int(10*random_search.best_params_[\"colsample_bytree\"])-1, int(10*random_search.best_params_[\"colsample_bytree\"])+2, 1))/10,\n",
    "        'min_child_weight': np.array(range(int(10*random_search.best_params_[\"min_child_weight\"])-1, int(10*random_search.best_params_[\"min_child_weight\"])+2, 1))/10\n",
    "    }\n",
    "    #'gamma': np.array(range(int(10*random_search.best_params_[\"gamma\"])-3, int(10*random_search.best_params_[\"gamma\"])+3, 1))/10,\n",
    "\n",
    "    fit_params = { \n",
    "            'early_stopping_rounds':10,\n",
    "            'eval_set':[(xvalid, valid_y)]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=xgb_pipeline, param_grid=gbm_param_grid, n_jobs=-1, cv=3,\n",
    "                             scoring='precision', verbose=10 )\n",
    "\n",
    "    grid_search.fit(xtrain,train_y,**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'learning_rate': 0.04, 'max_depth': 11, 'min_child_weight': 0.9, 'reg_lambda': 0.7, 'subsample': 0.9}\n",
      "0.9508729192042225\n"
     ]
    }
   ],
   "source": [
    "if grid_search_model:\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.score(xvalid,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_3\">III - Best Model</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_metrics(y_true, y_pred):\n",
    "    print(f\"Accuracy \\t\\t: {round(100*sklearn.metrics.accuracy_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"Kappa   \\t\\t: {round(100*sklearn.metrics.cohen_kappa_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"f1-score \\t\\t: {round(100*sklearn.metrics.f1_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"Precision \\t\\t: {round(100*sklearn.metrics.precision_score( y_true, y_pred),2)}%\")\n",
    "    print(f\"Recall  \\t\\t: {round(100*sklearn.metrics.recall_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"ROC AUC \\t\\t: {round(100*sklearn.metrics.roc_auc_score( y_true, y_pred),2)}%\")\n",
    "    print(f\"Matthews Corrcoef \\t: {round(100*sklearn.metrics.matthews_corrcoef(  y_true, y_pred),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_1\">III - 1. Train best parameters</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0,\n",
       " 'reg_lambda': 0.8,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.03,\n",
       " 'gamma': 0,\n",
       " 'colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best parameters \n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.03, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.8, scale_pos_weight=1, subsample=1.0,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new model with best parameters and early stopping\n",
    "model = XGBClassifier(n_estimators=1000, **random_search.best_params_)\n",
    "model.fit(xtrain,train_y, eval_metric=\"logloss\",eval_set=[(xvalid, valid_y)],early_stopping_rounds=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \t\t: 96.45%\n",
      "Kappa   \t\t: 92.89%\n",
      "f1-score \t\t: 96.44%\n",
      "Precision \t\t: 96.46%\n",
      "Recall  \t\t: 96.43%\n",
      "ROC AUC \t\t: 96.45%\n",
      "Matthews Corrcoef \t: 92.89%\n"
     ]
    }
   ],
   "source": [
    "# See the metrics \n",
    "y_pred = model.predict(xtest)\n",
    "results_metrics(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparison default parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_basic = XGBClassifier(n_estimators=1000,subsample=0.8)\n",
    "model_basic.fit(xtrain,train_y, eval_metric=\"logloss\",eval_set=[(xvalid, valid_y)],early_stopping_rounds=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \t\t: 95.91%\n",
      "Kappa   \t\t: 91.81%\n",
      "f1-score \t\t: 95.88%\n",
      "Precision \t\t: 96.27%\n",
      "Recall  \t\t: 95.5%\n",
      "ROC AUC \t\t: 95.91%\n",
      "Matthews Corrcoef \t: 91.81%\n"
     ]
    }
   ],
   "source": [
    "y_pred_basic = model_basic.predict(xtest)\n",
    "results_metrics(test_y, y_pred_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_2\">III - 2. Save model</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = NAME_XGB_CLASSIFIER_MODEL\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_3\">III - 3. Load and test to compare metrics</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \t\t: 96.42%\n",
      "Kappa   \t\t: 92.83%\n",
      "f1-score \t\t: 96.42%\n",
      "Precision \t\t: 96.12%\n",
      "Recall  \t\t: 96.73%\n",
      "ROC AUC \t\t: 96.42%\n",
      "Matthews Corrcoef \t: 92.83%\n",
      "CPU times: user 4.58 s, sys: 391 ms, total: 4.97 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = NAME_COUNT_VECT_MODEL\n",
    "# load the model from disk\n",
    "preproc_model = pickle.load(open(filename, 'rb'))\n",
    "valid__x = preproc_model.transform(valid_x)\n",
    "\n",
    "filename = NAME_XGB_CLASSIFIER_MODEL\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_pred = loaded_model.predict(valid__x)\n",
    "\n",
    "results_metrics(valid_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_4\">VI - Feature Importance</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_4_1\">VI 1 - Part 4 1</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_4_2\">VI 2 - Part 4 2</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_conclusion\">Conclusion</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
