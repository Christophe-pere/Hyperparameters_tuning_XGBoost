{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Hyperparameters optimization XGBoost Classifier</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center><h2>Hypertuning, Training best parameters, save model, feature importance</h2></center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Contents</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#part_intro)\n",
    "- [I - Hyperparameters Optimization](#part_1)\n",
    "    - [1 - Libraries](#part_1_1)\n",
    "    - [2 - Load Data](#part_1_2)\n",
    "    - [3 - Stopwords data](#part_1_3)\n",
    "    - [4 - Preparation train test](#part_1_4)\n",
    "- [II - Hyperparameters](#part_2)\n",
    "    - [1 - RandomSearchCV](#part_2_1)\n",
    "    - [2 - GridSearchCV](#part_2_2)\n",
    "- [III - Best Model](#part_3)\n",
    "    - [III - 1. Train model with best parameters](#part_3_1)\n",
    "    - [III - 2. Save Model](#part_3_2)\n",
    "    - [III - 3. Load and test to compare metrics](#part_3_3)\n",
    "- [VI - Feature importance](#part_4)\n",
    "    - [Part VI - 1](#part_4_1)\n",
    "    - [Part VI - 2](#part_4_2)\n",
    "- [Conclusion](#part_conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_intro\">Introduction</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_1\">I - Hyperparameters Optimization</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_1\">I 1 - Libraries</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "/home/chris/.local/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#imported libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# ---- Call tqdm to see progress bar with pandas\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a method for data treatment \n",
    "count_vect         = False\n",
    "tf_idf             = True\n",
    "tf_idf_ngram       = False\n",
    "tf_idf_ngram_char  = False\n",
    "concat_methods     = False # concat data representation\n",
    "random_search_model= True\n",
    "grid_search_model  = False\n",
    "num_gpu            = len(tf.config.experimental.list_physical_devices('GPU'))   # detect the number of gpu\n",
    "# Name file \n",
    "NAME_COUNT_VECT_MODEL         = \"count_vect_model.sav\"\n",
    "NAME_TF_IDF_MODEL             = \"TF_IDF_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_MODEL       = \"TF_IDF_ngram_model.sav\"\n",
    "NAME_TF_IDF_NGRAM_CHAR_MODEL  = \"TF_IDF_ngram_chars_model.sav\"\n",
    "NAME_XGB_CLASSIFIER_MODEL     = \"XGBoost_classifier.sav\"\n",
    "NAME_SAVE_FILE                = \"model_notaire_test\" # put just the name the .csv will be added at the end\n",
    "root_dir                      = \"\"       # Place here the path where you want your models stored or use /path/to/your/folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory can not be created\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dir_name =  NAME_SAVE_FILE\n",
    "    os.makedirs(os.path.join(root_dir,dir_name))\n",
    "    print(\"Directory has been created\")\n",
    "except:\n",
    "    print(\"Directory can not be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_2\">I 2 - Load Data</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../path_to_your_train_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT   = \"column_containing_text\"\n",
    "LABEL  = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=[TEXT], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train[(train[LABEL]!=\"co\")|(train[LABEL]!=\"ft\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../path_to_your_test_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_3\">I 3 - Stopwords data</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count_vect:\n",
    "    stop_word = np.loadtxt(\"../stopwords/stopwords-fr.txt\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count_vect:\n",
    "    def remove_stop_words( x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)\n",
    "    \n",
    "    train.loc[:,TEXT+\"_sw\"] = train.loc[:,TEXT].progress_apply(lambda x : remove_stop_words(x, stop_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train[LABEL].isnull().sum()>0:\n",
    "    train.dropna(subset=[LABEL], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mails         0\n",
       "ref_folder    0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8ce8a8e99622>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[LABEL][train[LABEL]!=\"ft_notaire\"] = \"other\"\n",
      "<ipython-input-13-8ce8a8e99622>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[LABEL][test[LABEL]!=\"ft_notaire\"] = \"other\"\n"
     ]
    }
   ],
   "source": [
    "train[LABEL][train[LABEL]!=\"ft_notaire\"] = \"other\"\n",
    "test[LABEL][test[LABEL]!=\"ft_notaire\"] = \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_1_4\">I 4 - Preparation train test</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# ML classic \n",
    "if count_vect:\n",
    "    train_x, test_x, y_train, y_test = train_test_split(train[TEXT+\"_sw\"], train[LABEL], random_state=42, stratify=train[LABEL], test_size=0.2)\n",
    "else:\n",
    "    train_x, test_x, y_train, y_test = train_test_split(train[TEXT], train[LABEL], random_state=42, stratify=train[LABEL], test_size=0.2)\n",
    "\n",
    "train_x, test_x, y_train, y_test = train[TEXT].values, test[TEXT].values, train[LABEL].values, test[LABEL].values\n",
    "# Validation set\n",
    "train_x, valid_x, y_train, y_valid = train_test_split(train_x, y_train, random_state=42, stratify=y_train, test_size=0.2)\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(y_train)\n",
    "valid_y = encoder.transform(y_valid)\n",
    "test_y = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21545,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_4\"><h3>One-Hot encoding (CountVectorizing)</h3></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.4 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if count_vect:\n",
    "    # create a count vectorizer object \n",
    "    count_vect_ = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect_.fit(train[TEXT]+\"_sw\")\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain =  count_vect_.transform(train_x)\n",
    "    xvalid =  count_vect_.transform(valid_x)\n",
    "    xtest =  count_vect_.transform(test_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a id=\"part_6_5\"><h3>TF-IDF</h3></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "CPU times: user 33.6 s, sys: 1.66 s, total: 35.3 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if tf_idf:\n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect.transform(train_x)\n",
    "    xvalid =  tfidf_vect.transform(valid_x)\n",
    "    xtest  =  tfidf_vect.transform(test_x)\n",
    "    #xtest  =  tfidf_vect.transform(_x_test)\n",
    "    print(\"word level tf-idf done\")\n",
    "    \n",
    "if tf_idf_ngram:\n",
    "    # ngram level tf-idf \n",
    "    tfidf_vect_ngram_ = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram_.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect_ngram_.transform(train_x)\n",
    "    xvalid =  tfidf_vect_ngram_.transform(valid_x)\n",
    "    xtest =  tfidf_vect_ngram_.transform(test_x)\n",
    "    print(\"ngram level tf-idf done\")\n",
    "    \n",
    "if tf_idf_ngram_char:\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars_ = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) #token_pattern=r'\\w{1,}',\n",
    "    tfidf_vect_ngram_chars_.fit(train[TEXT])\n",
    "    xtrain =  tfidf_vect_ngram_chars_.transform(train_x) \n",
    "    xvalid =  tfidf_vect_ngram_chars_.transform(valid_x) \n",
    "    xtest  =  tfidf_vect_ngram_chars_.transform(test_x) \n",
    "    print(\"characters level tf-idf done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count_vect:\n",
    "    # save the model to disk\n",
    "    filename = os.path.join(root_dir,dir_name,NAME_COUNT_VECT_MODEL)\n",
    "    pickle.dump(count_vect_, open(filename, 'wb'))\n",
    "\n",
    "if tf_idf:\n",
    "    # save the model to disk\n",
    "    filename = os.path.join(root_dir,dir_name,NAME_TF_IDF_MODEL)\n",
    "    pickle.dump(tfidf_vect, open(filename, 'wb'))\n",
    "    \n",
    "if tf_idf_ngram:\n",
    "    # save the model to disk\n",
    "    filename = os.path.join(root_dir,dir_name,NAME_TF_IDF_NGRAM_MODEL)\n",
    "    pickle.dump(tfidf_ngram_, open(filename, 'wb'))\n",
    "    \n",
    "if tf_idf_ngram_char:\n",
    "    # save the model to disk\n",
    "    filename = os.path.join(root_dir,dir_name,NAME_TF_IDF_NGRAM_CHAR_MODEL)\n",
    "    pickle.dump(tfidf_vect_ngram_chars_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_2\">II - Hyperparameters</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_2_1\">II 1 - RandomizedSearchCV</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 80.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 116.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 162.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed: 209.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 307.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 350.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.04214\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.04214\n",
      "[2]\tvalidation_0-error:0.04214\n",
      "[3]\tvalidation_0-error:0.04232\n",
      "[4]\tvalidation_0-error:0.04232\n",
      "[5]\tvalidation_0-error:0.04232\n",
      "[6]\tvalidation_0-error:0.04232\n",
      "[7]\tvalidation_0-error:0.04232\n",
      "[8]\tvalidation_0-error:0.04232\n",
      "[9]\tvalidation_0-error:0.04232\n",
      "[10]\tvalidation_0-error:0.04232\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-error:0.04214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if random_search_model:\n",
    "    #For classification \n",
    "\n",
    "    #Random Search\n",
    "    if num_gpu>0:\n",
    "        xgb_pipeline =XGBClassifier(tree_method= 'gpu_hist')\n",
    "    else:\n",
    "        xgb_pipeline =XGBClassifier()\n",
    "        \n",
    "    params = {\n",
    "            'learning_rate': [0.03, 0.01, 0.003, 0.001],\n",
    "            'min_child_weight': [1,3, 5,7, 10],\n",
    "            'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "            'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10, 12, 14],\n",
    "            'reg_lambda':np.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])}\n",
    "\n",
    "    fit_params = { \n",
    "            'early_stopping_rounds':10,\n",
    "            'eval_set':[(xvalid, valid_y)]}\n",
    "\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_pipeline, param_distributions=params, n_iter=1000,\n",
    "                                       scoring=\"precision\", n_jobs=-1,  verbose=3, random_state=42, cv=3 )\n",
    "    random_search.fit(xtrain,train_y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 1.0, 'reg_lambda': 0.4, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.001, 'gamma': 5, 'colsample_bytree': 1.0}\n",
      "0.9854217854217854\n"
     ]
    }
   ],
   "source": [
    "if random_search_model:\n",
    "    #Print out best parameters\n",
    "    print(random_search.best_params_)\n",
    "    #Print out scores on validation set\n",
    "    print(random_search.score(xtest,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_2_2\">II 2 - GridSearchCV</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.6,\n",
       " 'reg_lambda': 0.6,\n",
       " 'min_child_weight': 1,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.03,\n",
       " 'gamma': 5,\n",
       " 'colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_model:\n",
    "    #Grid Search\n",
    "\n",
    "    if num_gpu>0:\n",
    "        xgb_pipeline =XGBClassifier(tree_method= 'gpu_hist')\n",
    "    else:\n",
    "        xgb_pipeline =XGBClassifier()\n",
    "\n",
    "    gbm_param_grid = {\n",
    "        'learning_rate': np.array(range(int(100*random_search.best_params_[\"learning_rate\"])-1, int(100*random_search.best_params_[\"learning_rate\"])+2, 1))/100,\n",
    "        'subsample': np.array(range(int(10*random_search.best_params_[\"subsample\"])-1, int(10*random_search.best_params_[\"subsample\"])+2, 1))/10,\n",
    "        'reg_lambda': np.array(range(int(10*random_search.best_params_[\"reg_lambda\"])-1, int(10*random_search.best_params_[\"reg_lambda\"])+2, 1))/10,\n",
    "        'max_depth':np.array(range(random_search.best_params_[\"max_depth\"]-1, random_search.best_params_[\"max_depth\"]+3, 1)),\n",
    "        'colsample_bytree': np.array(range(int(10*random_search.best_params_[\"colsample_bytree\"])-1, int(10*random_search.best_params_[\"colsample_bytree\"])+2, 1))/10,\n",
    "        'min_child_weight': np.array(range(int(10*random_search.best_params_[\"min_child_weight\"])-1, int(10*random_search.best_params_[\"min_child_weight\"])+2, 1))/10\n",
    "    }\n",
    "    #'gamma': np.array(range(int(10*random_search.best_params_[\"gamma\"])-3, int(10*random_search.best_params_[\"gamma\"])+3, 1))/10,\n",
    "\n",
    "    fit_params = { \n",
    "            'early_stopping_rounds':10,\n",
    "            'eval_set':[(xvalid, valid_y)]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=xgb_pipeline, param_grid=gbm_param_grid, n_jobs=-1, cv=3,\n",
    "                             scoring='precision', verbose=10 )\n",
    "\n",
    "    grid_search.fit(xtrain,train_y,**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search_model:\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.score(xvalid,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_3\">III - Best Model</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_metrics(y_true, y_pred):\n",
    "    print(f\"Accuracy \\t\\t: {round(100*sklearn.metrics.accuracy_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"Kappa   \\t\\t: {round(100*sklearn.metrics.cohen_kappa_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"f1-score \\t\\t: {round(100*sklearn.metrics.f1_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"Precision \\t\\t: {round(100*sklearn.metrics.precision_score( y_true, y_pred),2)}%\")\n",
    "    print(f\"Recall  \\t\\t: {round(100*sklearn.metrics.recall_score(y_true, y_pred),2)}%\")\n",
    "    print(f\"ROC AUC \\t\\t: {round(100*sklearn.metrics.roc_auc_score( y_true, y_pred),2)}%\")\n",
    "    print(f\"Matthews Corrcoef \\t: {round(100*sklearn.metrics.matthews_corrcoef(  y_true, y_pred),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_1\">III - 1. Train best parameters</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0,\n",
       " 'reg_lambda': 0.4,\n",
       " 'min_child_weight': 5,\n",
       " 'max_depth': 5,\n",
       " 'learning_rate': 0.001,\n",
       " 'gamma': 5,\n",
       " 'colsample_bytree': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best parameters \n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0, gamma=5, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.001, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.4, scale_pos_weight=1, subsample=1.0,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new model with best parameters and early stopping\n",
    "model = XGBClassifier(n_estimators=1000, **random_search.best_params_)\n",
    "model.fit(xtrain,train_y, eval_metric=\"logloss\",eval_set=[(xvalid, valid_y)],early_stopping_rounds=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \t\t: 95.68%\n",
      "Kappa   \t\t: 88.03%\n",
      "f1-score \t\t: 97.17%\n",
      "Precision \t\t: 96.19%\n",
      "Recall  \t\t: 98.17%\n",
      "ROC AUC \t\t: 93.07%\n",
      "Matthews Corrcoef \t: 88.11%\n"
     ]
    }
   ],
   "source": [
    "# See the metrics \n",
    "y_pred = model.predict(xtest)\n",
    "results_metrics(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparison default parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_basic = XGBClassifier(n_estimators=1000,subsample=0.8)\n",
    "model_basic.fit(xtrain,train_y, eval_metric=\"logloss\",eval_set=[(xvalid, valid_y)],early_stopping_rounds=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \t\t: 97.55%\n",
      "Kappa   \t\t: 93.41%\n",
      "f1-score \t\t: 98.38%\n",
      "Precision \t\t: 98.62%\n",
      "Recall  \t\t: 98.14%\n",
      "ROC AUC \t\t: 96.94%\n",
      "Matthews Corrcoef \t: 93.41%\n"
     ]
    }
   ],
   "source": [
    "y_pred_basic = model_basic.predict(xtest)\n",
    "results_metrics(test_y, y_pred_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_2\">III - 2. Save model</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = os.path.join(root_dir,dir_name,NAME_XGB_CLASSIFIER_MODEL)\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_3_3\">III - 3. Load and test to compare metrics</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename = os.path.join(root_dir,dir_name,NAME_COUNT_VECT_MODEL)\n",
    "# load the model from disk\n",
    "preproc_model = pickle.load(open(filename, 'rb'))\n",
    "valid__x = preproc_model.transform(test_x)\n",
    "\n",
    "filename = os.path.join(root_dir,dir_name,NAME_XGB_CLASSIFIER_MODEL)\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_pred = loaded_model.predict(valid__x)\n",
    "\n",
    "results_metrics(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_4\">VI - Feature Importance</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_4_1\">VI 1 - Part 4 1</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"part_4_2\">VI 2 - Part 4 2</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"part_conclusion\">Conclusion</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
